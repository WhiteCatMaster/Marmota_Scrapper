import os
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO
import zipfile
import csv

# Ruta absoluta donde Kavita guarda la librer√≠a
LIBRARY_PATH = "/home/ubuntu/comics/kavita/data"
# Nombre del archivo para guardar los enlaces
URL_FILE_NAME = "comic_urls.csv"

def scrape_chapter(url, output_name):
    temp_dir = "temp_images"
    os.makedirs(temp_dir, exist_ok=True)

    print(f"üì• Descargando desde: {url}")
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error al conectar con la URL {url}: {e}")
        return

    soup = BeautifulSoup(response.text, "html.parser")
    img_tags = soup.find_all("img", {"data-src": True})
    images = []

    print(f"üîé Encontradas {len(img_tags)} im√°genes en la p√°gina...")

    for i, img in enumerate(img_tags):
        src = img.get("data-src", "").strip()
        if not src:
            continue
        try:
            r = requests.get(src)
            r.raise_for_status()
            image = Image.open(BytesIO(r.content)).convert("RGB")
            filename = os.path.join(temp_dir, f"{i:03}.jpg")
            image.save(filename, "JPEG")
            images.append(filename)
            print(f" ¬† ‚úÖ Descargada la p√°gina {i}: {filename}")
        except Exception as e:
            print(f" ¬† ‚ö†Ô∏è Error con {src}: {e}")

    if not images:
        print("ü§∑‚Äç‚ôÇÔ∏è No se encontraron im√°genes v√°lidas para descargar.")
        return

    os.makedirs(LIBRARY_PATH, exist_ok=True)

    output_path = os.path.join(LIBRARY_PATH, f"{output_name}.cbz")
    with zipfile.ZipFile(output_path, 'w') as cbz:
        for img in sorted(images):
            cbz.write(img, os.path.basename(img))

    print(f"üìö Archivo CBZ creado con √©xito: {output_path}")

    for f in images:
        os.remove(f)
    os.rmdir(temp_dir)
    print("üßπ Archivos temporales eliminados.")

def scrape_multiple(urls, comic_name, comic_year):
    if not urls:
        print("‚ùó No se han proporcionado URLs. Saliendo.")
        return
    for idx, url in enumerate(urls, start=1):
        chapter_name = f"{comic_name} ({comic_year}) - #{idx}"
        scrape_chapter(url, chapter_name)
        print("-" * 40)

def read_urls_from_file():
    urls = []
    if os.path.exists(URL_FILE_NAME):
        with open(URL_FILE_NAME, mode='r', newline='', encoding='utf-8') as file:
            reader = csv.reader(file)
            for row in reader:
                if len(row) >= 2:
                    urls.append((row[0], row[1])) # Almacena tuplas de (nombre, url)
    return urls

def append_to_file(comic_name, url):
    with open(URL_FILE_NAME, mode='a', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow([comic_name, url])

if __name__ == "__main__":
    
    # 1. Lee los enlaces existentes del archivo CSV
    urls_from_file = read_urls_from_file()
    if urls_from_file:
        print(f"‚úÖ Se encontraron {len(urls_from_file)} URL(s) en '{URL_FILE_NAME}'.")
        
        # Procesa los c√≥mics del archivo
        # Asume que todos los c√≥mics del archivo son de la misma serie
        comic_name, _ = urls_from_file[0]
        comic_urls_only = [url for _, url in urls_from_file]
        print("Iniciando descarga de c√≥mics desde el archivo...")
        scrape_multiple(comic_urls_only, comic_name, "2024")

    # 2. Pide nuevos enlaces al usuario y los a√±ade al archivo y a la lista de descargas
    print("\nPor favor, introduce nuevas URLs para descargar.")
    print("Cuando termines, escribe 'fin' y presiona Enter.")
    
    urls_to_scrape = []
    comic_name_input = input("¬øCu√°l es el nombre del c√≥mic?: ")
    
    while True:
        link = input(f"Ingresa la URL del cap√≠tulo #{len(urls_to_scrape) + 1}: ")
        if link.lower() == 'fin':
            break
        if link:
            urls_to_scrape.append(link)
            append_to_file(comic_name_input, link)
            
    # Procesa las nuevas URLs introducidas
    if urls_to_scrape:
        print("\nIniciando descarga de los nuevos c√≥mics...")
        scrape_multiple(urls_to_scrape, comic_name_input, "2024")

    print("\nProceso de scraping finalizado.")